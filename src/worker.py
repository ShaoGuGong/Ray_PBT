import logging
import os
import random
from typing import Dict, List, Optional, Union

import ray
import torch
import torch.nn as nn
import torch.optim as optim
from ray.actor import ActorHandle
from torch.utils.data import DataLoader

from .config import GPU_MAX_ITERATION, MUTATION_ITERATION
from .trial_state import TrialState
from .utils import (Accuracy, TrainStepFunction, TrialStatus, WorkerState,
                    WorkerType, get_data_loader, get_head_node_address,
                    get_model)


class WorkerLoggerFormatter(logging.Formatter):
    """
    è‡ªè¨‚çš„æ—¥èªŒæ ¼å¼å™¨ï¼Œç”¨æ–¼åœ¨æ—¥èªŒä¸­åŠ å…¥ worker_id å’Œ trial_idã€‚

    Attributes:
        None
    """

    def format(self, record):
        """
        æ ¼å¼åŒ–æ—¥èªŒç´€éŒ„ï¼ŒåŠ å…¥ worker_id å’Œ trial_idã€‚

        Args:
            record (logging.LogRecord): æ—¥èªŒç´€éŒ„ã€‚

        Returns:
            str: æ ¼å¼åŒ–å¾Œçš„æ—¥èªŒè¨Šæ¯ã€‚
        """

        worker_type = getattr(record, "worker_type", WorkerType.CPU)
        if worker_type == WorkerType.GPU:
            record.worker_type = "GPU"
        elif worker_type == WorkerType.CPU:
            record.worker_type = "CPU"

        record.worker_id = getattr(record, "worker_id", "N/A")
        record.trial_id = getattr(record, "trial_id", "N/A")
        return super().format(record)


def get_worker_logger(worker_id: int) -> logging.Logger:
    """
    å»ºç«‹ä¸¦å›å‚³æŒ‡å®š worker_id çš„ loggerï¼Œæ”¯æ´çµ‚ç«¯è¼¸å‡ºèˆ‡æª”æ¡ˆå¯«å…¥ã€‚

    Args:
        worker_id (int): Worker çš„å”¯ä¸€è­˜åˆ¥ç¢¼ã€‚

    Returns:
        logging.Logger: è¨­å®šå®Œæˆçš„ loggerã€‚
    """

    log_dir = os.path.join(os.getcwd(), "logs")
    os.makedirs(log_dir, exist_ok=True)

    logger = logging.getLogger(f"worker-{worker_id}")

    if not logger.handlers:
        logger.setLevel(logging.DEBUG)

        formatter = WorkerLoggerFormatter(
            "[%(asctime)s] %(levelname)s %(worker_type)s WORKER_ID: %(worker_id)s TRIAL_ID: %(trial_id)s -- %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )

        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.INFO)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

        file_handler = logging.FileHandler(
            os.path.join(log_dir, f"worker-{worker_id}.log")
        )
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


@ray.remote
class Worker:
    """
    è¡¨ç¤ºä¸€å€‹ worker ç¯€é»ï¼Œè² è²¬è¨“ç·´èˆ‡å›å ±è©¦é©—çµæœã€‚

    Attributes:
        worker_state (WorkerState): Worker çš„ç‹€æ…‹è³‡è¨Šã€‚
        active_trials (dict): æ´»èºè©¦é©—çš„å­—å…¸ã€‚
        train_step (TrainStepFunction): åŸ·è¡Œè¨“ç·´æ­¥é©Ÿçš„å‡½å¼ã€‚
        device (torch.device): ä½¿ç”¨çš„è¨­å‚™ï¼ˆCPU æˆ– GPUï¼‰ã€‚
        logger (logging.Logger): è² è²¬æ—¥èªŒç´€éŒ„ã€‚
    """

    def __init__(
        self,
        worker_state: WorkerState,
        train_step: TrainStepFunction,
        tuner: ActorHandle,
    ) -> None:
        """
        åˆå§‹åŒ– Workerï¼Œè¨­å®šç‹€æ…‹èˆ‡åƒæ•¸ã€‚

        Args:
            worker_state (WorkerState): Worker çš„ç‹€æ…‹è³‡è¨Šã€‚
            train_step (TrainStepFunction): è¨“ç·´æ­¥é©Ÿå‡½å¼ã€‚
            tuner (ActorHandle): è² è²¬æ¥æ”¶è¨“ç·´çµæœçš„ Actorã€‚
        """
        self.worker_state: WorkerState = worker_state
        self.active_trials: Dict[int, TrialState] = {}
        self.train_step = train_step
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger = get_worker_logger(worker_id=worker_state.id)
        self.log("info", "åˆå§‹åŒ–å®Œæˆ")
        self.tuner = tuner
        self.mutation_iteration: int = MUTATION_ITERATION
        self.interrupt_table: List[int] = []
        self.signals: Dict[int, bool] = {}

    def send_signal(self, trial_id):
        if trial_id not in self.active_trials:
            return
        self.log("info", f"æ¥æ”¶åˆ°è¨Šè™Ÿ trial: {trial_id}")
        self.signals[trial_id] = True

    def assign_trial(self, trial_state: TrialState) -> Optional[TrialState]:
        """
        å°‡è©¦é©—åˆ†é…çµ¦è©² worker ä¸¦é–‹å§‹è¨“ç·´ã€‚

        Args:
            trial_state (TrialState): è©¦é©—ç‹€æ…‹ã€‚

        Returns:
            TrialState: æ›´æ–°å¾Œçš„è©¦é©—ç‹€æ…‹ã€‚
        """
        if len(self.active_trials) >= self.worker_state.max_trials:
            return trial_state

        self.active_trials[trial_state.id] = trial_state
        trial_state.status = TrialStatus.RUNNING
        trial_state.worker_id = self.worker_state.id
        trial_state.worker_type = self.worker_state.worker_type
        self.log("info", f"åŸ·è¡Œä¸­Trial: {[i for i in self.active_trials]}")
        return self.train(trial_state)

    def get_active_trials_nums(self) -> int:
        """
        å–å¾—ç›®å‰æ´»èºè©¦é©—çš„æ•¸é‡ã€‚

        Returns:
            int: æ´»èºè©¦é©—æ•¸é‡ã€‚
        """
        return len(self.active_trials)

    def get_active_trials(self) -> List[TrialState]:
        return list(self.active_trials.values())

    def get_available_slots(self) -> int:
        """
        å–å¾—å¯ä¾›åˆ†é…çš„æ–°è©¦é©—æ’æ§½æ•¸ã€‚

        Returns:
            int: å¯åˆ†é…è©¦é©—çš„æ’æ§½æ•¸ã€‚
        """
        return self.worker_state.max_trials - len(self.active_trials)

    def train(self, trial_state: TrialState) -> TrialState:
        """
        åŸ·è¡Œè©¦é©—çš„è¨“ç·´æµç¨‹ã€‚

        Args:
            trial_state (TrialState): è©¦é©—ç‹€æ…‹ã€‚

        Returns:
            TrialState: è¨“ç·´å¾Œçš„è©¦é©—ç‹€æ…‹ã€‚
        """
        self.log("info", "é–‹å§‹è¨“ç·´", trial_id=trial_state.id)

        hyper = trial_state.hyperparameter
        checkpoint = trial_state.checkpoint
        train_loader, test_loader = get_data_loader(hyper.model_type, hyper.batch_size)

        model = get_model(hyper.model_type)
        model.load_state_dict(checkpoint.model_state_dict)
        model.to(self.device)

        current_iteration = 0

        optimizer = optim.SGD(model.parameters(), lr=hyper.lr, momentum=hyper.momentum)
        optimizer.load_state_dict(checkpoint.optimzer_state_dict)
        for param_group in optimizer.param_groups:
            param_group["lr"] = hyper.lr
            param_group["momentum"] = hyper.momentum

        while True:
            if trial_state.accuracy > trial_state.stop_accuracy:
                break

            if trial_state.iteration >= trial_state.stop_iteration:
                trial_state.accuracy = self.test(model, test_loader)
                break

            if self.signals.get(trial_state.id, False):
                self.log("info", "æ”¶åˆ°å›å‚³è¨Šè™Ÿ")
                self.pause_trial(trial_state)
                self.signals.pop(trial_state.id, None)
                return trial_state

            if trial_state.id in self.interrupt_table:
                self.interrupt_table.remove(trial_state.id)
                self.pause_trial(trial_state)
                return trial_state

            self.train_step(
                model, optimizer, train_loader, hyper.batch_size, self.device
            )

            trial_state.iteration += 1
            current_iteration += 1

            if trial_state.iteration % self.mutation_iteration:
                continue

            if (
                self.worker_state.worker_type == WorkerType.GPU
                and current_iteration >= GPU_MAX_ITERATION
            ):
                self.pause_trial(trial_state)
                return trial_state

            trial_state.accuracy = self.test(model, test_loader)

            self.log(
                "info",
                f"Iteration: {trial_state.iteration} Accuracy: {trial_state.accuracy}",
                trial_id=trial_state.id,
            )

            ray.get(self.tuner.update_trial_result.remote(trial_state))

            base_line = ray.get(
                self.tuner.get_baseline.remote(iteration=trial_state.iteration)
            )

            if trial_state.accuracy >= base_line:
                continue

            self.log(
                "info",
                f"Accuracy:{trial_state.accuracy:.4f} < Base Line:{base_line:.4f}",
                trial_id=trial_state.id,
            )

            if random.choice((False, False, True)):
                self.log(
                    "info",
                    f"ğŸš« è¨“ç·´ä¸­æ­¢ä¸¦å›å‚³",
                    trial_id=trial_state.id,
                )

                self.pause_trial(trial_state)
                return trial_state

        self.log("info", f"è¨“ç·´çµæŸ", trial_id=trial_state.id)
        self.finish_trial(trial_state)

        return trial_state

    def test(self, model: nn.Module, test_loader: DataLoader) -> Accuracy:
        """
        ä½¿ç”¨æ¸¬è©¦è³‡æ–™å°æ¨¡å‹é€²è¡Œæ¸¬è©¦ä¸¦å›å‚³æº–ç¢ºç‡ã€‚

        Args:
            model (torch.nn.Module): å·²è¨“ç·´çš„æ¨¡å‹ã€‚
            test_loader (torch.utils.data.DataLoader): æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨ã€‚

        Returns:
            Accuracy: æ¨¡å‹æ¸¬è©¦çµæœçš„æº–ç¢ºç‡ã€‚
        """
        model.eval()
        total = 0
        correct = 0
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        return correct / total

    def finish_trial(self, trial_state: TrialState) -> None:
        """
        å°‡è©¦é©—æ¨™è¨˜ç‚ºçµ‚æ­¢ä¸¦å¾æ´»èºåˆ—è¡¨ä¸­ç§»é™¤ã€‚

        Args:
            trial_state (TrialState): è©¦é©—ç‹€æ…‹ã€‚
        """
        self.active_trials.pop(trial_state.id)
        trial_state.status = TrialStatus.TERMINATE

    def pause_trial(self, trial_state: TrialState) -> None:
        """
        å°‡è©¦é©—æ¨™è¨˜ç‚ºæš«åœä¸¦å¾æ´»èºåˆ—è¡¨ä¸­ç§»é™¤ã€‚

        Args:
            trial_state (TrialState): è©¦é©—ç‹€æ…‹ã€‚
        """
        self.active_trials.pop(trial_state.id)
        trial_state.status = TrialStatus.PAUSE

    def get_log_file(self) -> Dict[str, Union[int, str]]:
        """
        å–å¾— worker å°æ‡‰çš„æ—¥èªŒæª”æ¡ˆå…§å®¹ã€‚

        Returns:
            dict: åŒ…å« worker ID èˆ‡å°æ‡‰æ—¥èªŒå…§å®¹çš„å­—å…¸ã€‚
        """
        log_dir = None
        for handler in self.logger.handlers:
            if isinstance(handler, logging.FileHandler):
                log_dir = handler.baseFilename
                break

        if not log_dir:
            self.log("error", "Logs direction is not exists")
            return {"id": self.worker_state.id, "content": ""}

        with open(log_dir, "r") as f:
            return {"id": self.worker_state.id, "content": f.read()}

    def log(self, level: str, message: str, trial_id: Union[int, str] = "N/A") -> None:
        """
        æ ¹æ“šæŒ‡å®šçš„ log ç´šåˆ¥è¼¸å‡ºè¨Šæ¯ã€‚

        Args:
            level (str): è¨˜éŒ„ç­‰ç´šï¼ˆinfo/debug/warning/error/criticalï¼‰ã€‚
            message (str): è¦è¨˜éŒ„çš„è¨Šæ¯ã€‚
            trial_id (Union[int, str], optional): è©¦é©— IDã€‚é è¨­ç‚º "N/A"ã€‚
        """
        extra = {
            "worker_type": self.worker_state.worker_type,
            "worker_id": self.worker_state.id,
            "trial_id": trial_id,
        }
        if level == "info":
            self.logger.info(message, extra=extra)
            return
        if level == "debug":
            self.logger.info(message, extra=extra)
            return
        if level == "warning":
            self.logger.warning(message, extra=extra)
            return
        if level == "critical":
            self.logger.critical(message, extra=extra)
            return
        if level == "error":
            self.logger.error(message, extra=extra)
            return

    def get_worker_type(self) -> WorkerType:
        """
        å›å‚³ worker é¡å‹ï¼ˆCPU/GPUï¼‰ã€‚

        Returns:
            WorkerType: Worker é¡å‹ã€‚
        """
        return self.worker_state.worker_type


def generate_all_workers(
    tuner: ActorHandle, train_step: TrainStepFunction
) -> List[ActorHandle]:
    """
    æ ¹æ“š Ray å¢é›†çš„ç¯€é»è³‡æºå»ºç«‹æ‰€æœ‰ Workerã€‚

    Args:
        tuner (ActorHandle): æ¥æ”¶è©¦é©—çµæœçš„ Actorã€‚
        train_step (TrainStepFunction): è¨“ç·´æ­¥é©Ÿå‡½å¼ã€‚

    Returns:
        List[ActorHandle]: å»ºç«‹çš„ Worker Actor æ¸…å–®ã€‚
    """

    visited_address = set()
    worker_states = []
    index = 0
    head_node_address = get_head_node_address()
    print(head_node_address)

    for node in ray.nodes():
        node_address = node["NodeManagerAddress"]

        if node["Alive"]:
            if node_address in visited_address:
                continue

            resource = node["Resources"]
            if "CPU" in resource:
                if node_address == head_node_address:
                    cpus = min(resource.get("CPU", 1) - 1, 1)
                else:
                    cpus = resource.get("CPU", 1)

                worker_states.append(
                    WorkerState(
                        id=index,
                        num_cpus=cpus,
                        num_gpus=0,
                        node_name=f"node:{node_address}",
                        max_trials=1,
                        worker_type=WorkerType.CPU,
                    )
                )
                index += 1

            if "GPU" in resource:
                worker_states.append(
                    WorkerState(
                        id=index,
                        num_cpus=0,
                        num_gpus=resource.get("GPU", 0),
                        node_name=f"node:{node_address}",
                        max_trials=3,
                        worker_type=WorkerType.GPU,
                    )
                )
                index += 1
            visited_address.add(node_address)

    workers: list[ActorHandle] = []
    print(*worker_states, sep="\n")

    for index, worker_state in enumerate(worker_states):
        workers.append(
            Worker.options(
                max_concurrency=worker_state.max_trials + 1,
                name=f"worker-{index}",
                num_cpus=worker_state.num_cpus,
                num_gpus=worker_state.num_gpus,
                resources={worker_state.node_name: 0.01},
            ).remote(worker_state, train_step=train_step, tuner=tuner)
        )

    return workers
