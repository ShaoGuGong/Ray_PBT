import logging
import os
import random
from typing import Dict, List, Union

import ray
import torch
import torch.optim as optim
from ray.actor import ActorHandle

from .config import MUTATION_ITERATION
from .trial_state import TrialState
from .utils import (Accuracy, TrainStepFunction, TrialStatus, WorkerState,
                    get_data_loader, get_head_node_address, get_model)


class WorkerLoggerFormatter(logging.Formatter):
    """
    è‡ªå®šç¾©çš„æ—¥èªŒæ ¼å¼åŒ–å™¨ï¼Œç‚ºæ—¥èªŒæ·»åŠ  worker_id å’Œ trial_idã€‚

    Attributes:
        None
    """

    def format(self, record):
        """
        æ ¼å¼åŒ–æ—¥èªŒä¿¡æ¯ï¼Œä¸¦æ·»åŠ  worker_id å’Œ trial_idã€‚

        Args:
            record (logging.LogRecord): æ—¥èªŒè¨˜éŒ„ã€‚

        Returns:
            str: æ ¼å¼åŒ–çš„æ—¥èªŒæ¶ˆæ¯ã€‚
        """
        record.worker_id = getattr(record, "worker_id", "N/A")
        record.trial_id = getattr(record, "trial_id", "N/A")
        return super().format(record)


def get_worker_logger(worker_id: int) -> logging.Logger:
    """
    å‰µå»ºä¸¦è¿”å›ä¸€å€‹é‡å°ç‰¹å®š worker_id çš„ loggerï¼Œæ”¯æŒçµ‚ç«¯è¼¸å‡ºèˆ‡æ–‡ä»¶å¯«å…¥ã€‚

    Args:
        worker_id (int): å·¥ä½œè€…çš„å”¯ä¸€ IDã€‚

    Returns:
        logging.Logger: é…ç½®å¥½çš„ loggerã€‚
    """

    # ç¢ºä¿ logs ç›®éŒ„å­˜åœ¨
    log_dir = os.path.join(os.getcwd(), "logs")
    os.makedirs(log_dir, exist_ok=True)

    # ä½¿ç”¨ worker_id å‰µå»ºå”¯ä¸€çš„ logger åç¨±
    logger = logging.getLogger(f"worker-{worker_id}")

    # é˜²æ­¢é‡è¤‡æ·»åŠ  handler
    if not logger.handlers:
        # è¨­å®š logger çš„æœ€åŸºæœ¬ç´šåˆ¥
        logger.setLevel(logging.DEBUG)  # æˆ–è€…é¸æ“‡æ›´åˆé©çš„ç´šåˆ¥

        # çµ±ä¸€æ ¼å¼è¨­å®š
        formatter = WorkerLoggerFormatter(
            "[%(asctime)s] %(levelname)s WORKER_ID: %(worker_id)s TRIAL_ID: %(trial_id)s -- %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )

        # è¨­å®š stream handler (åªé¡¯ç¤ºåœ¨çµ‚ç«¯)
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.INFO)  # åªé¡¯ç¤º INFO ç´šåˆ¥ä»¥ä¸Šçš„è¨Šæ¯
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

        # è¨­å®š file handler (å°‡æ—¥èªŒå¯«å…¥æ–‡ä»¶)
        file_handler = logging.FileHandler(
            os.path.join(log_dir, f"worker-{worker_id}.log")
        )
        file_handler.setLevel(logging.DEBUG)  # è¨˜éŒ„æ‰€æœ‰ç´šåˆ¥çš„æ—¥èªŒ
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


@ray.remote
class Worker:
    """
    ä»£è¡¨ä¸€å€‹å·¥ä½œè€…ç¯€é»çš„é¡ï¼Œè² è²¬è¨“ç·´ä¸€å€‹è©¦é©—ä¸¦æ›´æ–°å…¶çµæœã€‚

    Attributes:
        worker_state (WorkerState): å·¥ä½œè€…çš„ç‹€æ…‹ä¿¡æ¯ã€‚
        active_trials (dict): æ´»èºçš„è©¦é©—å­—å…¸ï¼Œéµç‚ºè©¦é©— IDï¼Œå€¼ç‚ºè©¦é©—ç‹€æ…‹ã€‚
        train_step (TrainStepFunction): è¨“ç·´æ­¥é©Ÿå‡½æ•¸ã€‚
        device (torch.device): è¨“ç·´æ‰€ä½¿ç”¨çš„è¨­å‚™ï¼ˆCUDA æˆ– CPUï¼‰ã€‚
        logger (logging.Logger): ç”¨æ–¼è¨˜éŒ„è¨“ç·´éç¨‹çš„ loggerã€‚
    """

    def __init__(
        self,
        worker_state: WorkerState,
        train_step: TrainStepFunction,
        tuner: ActorHandle,
    ) -> None:
        """
        åˆå§‹åŒ– Worker é¡ï¼Œä¸¦è¨­ç½®ç›¸é—œå±¬æ€§ã€‚

        Args:
            worker_state (WorkerState): å·¥ä½œè€…çš„ç‹€æ…‹ä¿¡æ¯ã€‚
            train_result (ActorHandle): ç”¨æ–¼æ›´æ–°è¨“ç·´çµæœçš„ Actorã€‚
            train_step (TrainStepFunction): è¨“ç·´æ­¥é©Ÿå‡½æ•¸ã€‚
        """

        self.worker_state: WorkerState = worker_state
        self.active_trials = {}
        self.train_step = train_step
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger = get_worker_logger(worker_id=worker_state.id)
        self.log("info", "åˆå§‹åŒ–å®Œæˆ")
        self.tuner = tuner
        self.mutation_iteration: int = MUTATION_ITERATION

    def assign_trial(self, trial_state: TrialState) -> TrialState:
        """
        åˆ†é…ä¸€å€‹è©¦é©—çµ¦é€™å€‹å·¥ä½œè€…ï¼Œä¸¦é–‹å§‹è¨“ç·´ã€‚

        Args:
            trial_state (TrialState): è©¦é©—çš„ç‹€æ…‹ã€‚

        Returns:
            TrialState: æ›´æ–°å¾Œçš„è©¦é©—ç‹€æ…‹ã€‚
        """

        self.active_trials[trial_state.id] = trial_state
        trial_state.worker_id = self.worker_state.id
        self.log("info", f"åŸ·è¡Œä¸­Trial: {[i for i in self.active_trials]}")
        return self.train(trial_state)

    def get_active_trials_nums(self) -> int:
        """
        ç²å–ç›®å‰æ´»èºçš„è©¦é©—æ•¸é‡ã€‚

        Returns:
            int: æ´»èºè©¦é©—æ•¸é‡ã€‚
        """
        return len(self.active_trials)

    def has_available_slots(self) -> bool:
        """
        åˆ¤æ–·é€™å€‹å·¥ä½œè€…æ˜¯å¦é‚„æœ‰å¯ç”¨çš„æ’æ§½ä¾†è™•ç†æ›´å¤šçš„è©¦é©—ã€‚

        Returns:
            bool: å¦‚æœæœ‰å¯ç”¨æ’æ§½å‰‡è¿”å› Trueï¼Œå¦å‰‡è¿”å› Falseã€‚
        """
        return len(self.active_trials) < self.worker_state.max_trials

    def train(self, trial_state: TrialState) -> TrialState:
        """
        åŸ·è¡Œä¸€å€‹è¨“ç·´éç¨‹ï¼Œä¸¦æ›´æ–°è©¦é©—ç‹€æ…‹ã€‚

        Args:
            trial_state (TrialState): è©¦é©—çš„ç‹€æ…‹ã€‚

        Returns:
            TrialState: æ›´æ–°å¾Œçš„è©¦é©—ç‹€æ…‹ã€‚
        """
        self.log("info", "é–‹å§‹è¨“ç·´", trial_id=trial_state.id)

        hyper = trial_state.hyperparameter
        checkpoint = trial_state.checkpoint
        train_loader, test_loader = get_data_loader(hyper.model_type, hyper.batch_size)

        model = get_model(hyper.model_type)
        model.load_state_dict(checkpoint.model_state_dict)
        model.to(self.device)

        optimizer = optim.SGD(model.parameters(), lr=hyper.lr, momentum=hyper.momentum)
        optimizer.load_state_dict(checkpoint.optimzer_state_dict)
        for param_group in optimizer.param_groups:
            param_group["lr"] = hyper.lr
            param_group["momentum"] = hyper.momentum

        while True:
            if trial_state.accuracy > trial_state.stop_accuracy:
                break

            if trial_state.iteration >= trial_state.stop_iteration:
                break

            self.train_step(
                model, optimizer, train_loader, hyper.batch_size, self.device
            )

            trial_state.accuracy = self.test(model, test_loader)

            trial_state.iteration += 1

            self.log(
                "info",
                f"Iteration: {trial_state.iteration} Accuracy: {trial_state.accuracy}",
                trial_id=trial_state.id,
            )

            if trial_state.iteration % self.mutation_iteration:
                continue

            ray.get(
                self.tuner.update_trial_result.remote(
                    iteration=trial_state.iteration,
                    accuracy=trial_state.accuracy,
                    hyperparameter=trial_state.hyperparameter,
                    checkpoint=trial_state.checkpoint,
                )
            )

            base_line = ray.get(
                self.tuner.get_mean_accuracy.remote(iteration=trial_state.iteration)
            )

            if trial_state.accuracy >= base_line:
                continue

            self.log(
                "info",
                f"Accuracy:{trial_state.accuracy:.4f} < Base Line:{base_line:.4f}",
                trial_id=trial_state.id,
            )

            if random.choice((False, False, True)):
                self.log(
                    "info",
                    f"ğŸš« è¨“ç·´ä¸­æ­¢ä¸¦å›å‚³",
                    trial_id=trial_state.id,
                )

                self.pause_trial(trial_state)
                return trial_state

        self.log("info", f"è¨“ç·´çµæŸ", trial_id=trial_state.id)
        self.finish_trial(trial_state)

        return trial_state

    def test(self, model, test_loader) -> Accuracy:
        """
        æ¸¬è©¦æ¨¡å‹ä¸¦è¨ˆç®—æº–ç¢ºç‡ã€‚

        Args:
            model (torch.nn.Module): è¨“ç·´å¥½çš„æ¨¡å‹ã€‚
            test_loader (torch.utils.data.DataLoader): æ¸¬è©¦æ•¸æ“šåŠ è¼‰å™¨ã€‚

        Returns:
            Accuracy: æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„æº–ç¢ºç‡ã€‚
        """
        model.eval()
        total = 0
        correct = 0
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        return correct / total

    def finish_trial(self, trial_state: TrialState) -> None:
        """
        çµæŸä¸¦æ¨™è¨˜ä¸€å€‹è©¦é©—çš„ç‹€æ…‹ç‚ºçµ‚æ­¢ã€‚

        Args:
            trial_state (TrialState): è©¦é©—ç‹€æ…‹ã€‚
        """

        self.active_trials.pop(trial_state.id)
        trial_state.status = TrialStatus.TERMINAL

    def pause_trial(self, trial_state: TrialState) -> None:
        self.active_trials.pop(trial_state.id)
        trial_state.status = TrialStatus.PAUSE

    def get_log_file(self) -> Dict[str, Union[int, str]]:
        """
        ç²å–å·¥ä½œè€…çš„æ—¥èªŒæ–‡ä»¶å…§å®¹ã€‚

        Returns:
            dict: åŒ…å« worker_id å’Œæ—¥èªŒå…§å®¹çš„å­—å…¸ã€‚
        """

        log_dir = None
        for handler in self.logger.handlers:
            if isinstance(handler, logging.FileHandler):
                log_dir = handler.baseFilename  # å–å¾—è³‡æ–™å¤¾è·¯å¾‘
                break

        if not log_dir:
            self.log("error", "Logs direction is not exists")
            return {"id": self.worker_state.id, "content": ""}

        with open(log_dir, "r") as f:
            return {"id": self.worker_state.id, "content": f.read()}

    def log(self, level: str, message: str, trial_id: Union[int, str] = "N/A") -> None:
        if level == "info":
            self.logger.info(
                message, extra={"worker_id": self.worker_state.id, "trial_id": trial_id}
            )
            return
        if level == "debug":
            self.logger.info(
                message, extra={"worker_id": self.worker_state.id, "trial_id": trial_id}
            )
            return
        if level == "warning":
            self.logger.warning(
                message, extra={"worker_id": self.worker_state.id, "trial_id": trial_id}
            )
            return
        if level == "critical":
            self.logger.critical(
                message, extra={"worker_id": self.worker_state.id, "trial_id": trial_id}
            )
            return
        if level == "error":
            self.logger.error(
                message, extra={"worker_id": self.worker_state.id, "trial_id": trial_id}
            )
            return


def generate_all_workers(
    tuner: ActorHandle, train_step: TrainStepFunction
) -> List[ActorHandle]:
    """
    æ ¹æ“š Ray é›†ç¾¤ä¸­çš„ç¯€é»è³‡æºå‰µå»ºå·¥ä½œè€…ï¼Œä¸¦è¿”å›å·¥ä½œè€… Actor åˆ—è¡¨ã€‚

    Args:
        train_result (ActorHandle): ç”¨æ–¼æ›´æ–°è¨“ç·´çµæœçš„ Actorã€‚
        train_step (TrainStepFunction): è¨“ç·´æ­¥é©Ÿå‡½æ•¸ã€‚

    Returns:
        List[ActorHandle]: å‰µå»ºçš„å·¥ä½œè€… Actor åˆ—è¡¨ã€‚
    """

    visited_address = set()
    worker_states = []
    index = 0
    head_node_address = get_head_node_address()
    print(head_node_address)

    # Fetch all avaiable resource from Ray cluster.
    for node in ray.nodes():
        node_address = node["NodeManagerAddress"]

        if node["Alive"]:
            if node_address in visited_address:
                continue

            resource = node["Resources"]
            if "CPU" in resource:
                if node_address == head_node_address:
                    cpus = min(resource.get("CPU", 1) - 1, 1)
                else:
                    cpus = resource.get("CPU", 1)
                # cpus = resource.get("CPU", 1)

                worker_states.append(
                    WorkerState(
                        id=index,
                        num_cpus=cpus,
                        num_gpus=0,
                        node_name=f"node:{node_address}",
                        max_trials=3,
                    )
                )
                index += 1

            if "GPU" in resource:
                worker_states.append(
                    WorkerState(
                        id=index,
                        num_cpus=0,
                        num_gpus=resource.get("GPU", 0),
                        node_name=f"node:{node_address}",
                        max_trials=3,
                    )
                )
                index += 1
            visited_address.add(node_address)

    workers: list[ActorHandle] = []
    print(*worker_states, sep="\n")

    for index, worker_state in enumerate(worker_states):
        workers.append(
            Worker.options(
                max_concurrency=worker_state.max_trials + 1,
                name=f"worker-{index}",
                num_cpus=worker_state.num_cpus,
                num_gpus=worker_state.num_gpus,
                resources={worker_state.node_name: 0.01},
            ).remote(worker_state, train_step=train_step, tuner=tuner)
        )

    return workers
